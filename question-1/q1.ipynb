{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the cats and dogs well separated, i.e. can you obtain good classification test accuracy performance on this data set? Compare at least 3 classifiers.\n",
    "\n",
    "Are there any images that are consistently mislabeled by the classifiers (use resampling to ascertain)? Why do you think these are difficult images to classify? Do the classifiers struggle with the same observations?\n",
    "\n",
    "Are the errors balanced or is one class more difficult to classify correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate, cross_val_predict\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def read_data_file(filename: str) -> pd.DataFrame:\n",
    "    path = Path(os.getcwd() + 'q1.ipynb')\n",
    "    data_folder = str(path.parent.absolute()) + '/data/'\n",
    "    return pd.read_csv(data_folder + filename)\n",
    "\n",
    "\n",
    "features = read_data_file('CATSnDOGS.csv') / 255 # Rescale features to values between 0 and 1\n",
    "labels = read_data_file('Labels.csv')\n",
    "\n",
    "models = [\n",
    "          ('SVM', SVC()),\n",
    "          ('LogReg', LogisticRegression(max_iter=1000)), \n",
    "          ('RF', RandomForestClassifier())\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename\n",
    "def evaluate_ensemble(models: list, features: pd.DataFrame, labels: pd.DataFrame) -> pd.DataFrame:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.25, stratify=labels, random_state=None)\n",
    "    dfs = []\n",
    "    scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "    for name, model in models:\n",
    "        kfold = StratifiedKFold(n_splits=9, shuffle=True, random_state=42)\n",
    "        cross_validate(model, X_train, y_train.to_numpy().ravel(), cv=kfold, scoring=scoring)\n",
    "        clf = model.fit(X_train, y_train.to_numpy().ravel())\n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        df = pd.DataFrame({'_id': y_test.index.values, 'true': y_test.to_numpy().ravel(), 'predicted': y_pred, 'model': name})\n",
    "        dfs.append(df)\n",
    "\n",
    "    result = pd.concat(dfs, ignore_index=True)\n",
    "    return result\n",
    "\n",
    "\n",
    "# want: mean and std for each scoring, for each model over iterations\n",
    "def average_scores(models: list, features: pd.DataFrame, labels: pd.DataFrame, iter: int=5):\n",
    "    matrix = []\n",
    "    for it in range(iter):\n",
    "        res = evaluate_ensemble(models, features, labels)\n",
    "        scores = calc_scores(res)\n",
    "        matrix.append(scores)\n",
    "    \n",
    "    mean = np.mean(matrix, axis=0)\n",
    "    std = np.std(matrix, axis=0)\n",
    "    return mean, std\n",
    "\n",
    "    \n",
    "# want: score for each scoring for each model\n",
    "def calc_scores(ensemble_result: pd.DataFrame) -> np.ndarray:\n",
    "    model_names = ensemble_result.model.unique()\n",
    "    scores = np.zeros((len(model_names), 3))\n",
    "\n",
    "    for i, name in enumerate(model_names):\n",
    "        this_df = ensemble_result[ensemble_result.model == name]\n",
    "        y_true = this_df['true']\n",
    "        y_pred = this_df['predicted']\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        cat_accuracy = tn / (tn + fp) # specificity\n",
    "        dog_accuracy = tp / (tp + fn) # recall\n",
    "        accuracy = (tn + tp)/(tn + fp + fn + tp)\n",
    "        scores[i][0] = cat_accuracy\n",
    "        scores[i][1] = dog_accuracy\n",
    "        scores[i][2] = accuracy\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def detect_mislabels(models: list, features: pd.DataFrame, labels: pd.DataFrame, iter: int = 5, vote_limit: float = 0.5) -> dict:  \n",
    "    dfs = []\n",
    "    mislabels = {}\n",
    "\n",
    "    for it in range(iter):\n",
    "        res = evaluate_ensemble(models, features, labels)\n",
    "        res['vote'] = np.absolute(res.true-res.predicted)\n",
    "        dfs.append(res)\n",
    "\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    model_names = df.model.unique()\n",
    "\n",
    "    for i, name in enumerate(model_names):\n",
    "        mislabelled_ids = []\n",
    "        model_df = df[df.model == name]\n",
    "        _ids = model_df._id.unique()\n",
    "\n",
    "        for _id in _ids:\n",
    "            _id_df = model_df[model_df._id == _id]\n",
    "            total = len(_id_df.index)\n",
    "            vote = _id_df['vote'].sum()\n",
    "            if vote/total < vote_limit:\n",
    "                continue\n",
    "            else:\n",
    "                mislabelled_ids.append(_id)\n",
    "\n",
    "        mislabels[name] = mislabelled_ids\n",
    "\n",
    "    return mislabels\n",
    "\n",
    "\n",
    "mean, std = average_scores(models, features, labels)\n",
    "mislabels = detect_mislabels(models, features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8, 139, 14, 21, 151, 26, 158, 37, 168, 176, 180, 53, 54, 58, 63, 192, 78, 79, 85, 90, 93, 96, 113, 114, 124}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "not enough arguments for format string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-ea163f432895>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mintersection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmislabels_svm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmislabels_logreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmislabels_rf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s \\% of mislabels by the SVM is shared by all models.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmislabels_svm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: not enough arguments for format string"
     ]
    }
   ],
   "source": [
    "mislabels_svm = set(mislabels['SVM'])\n",
    "mislabels_logreg = set(mislabels['LogReg'])\n",
    "mislabels_rf = set(mislabels['RF'])\n",
    "\n",
    "intersection = mislabels_svm.intersection(mislabels_logreg).intersection(mislabels_rf)\n",
    "print(intersection)\n",
    "print(\"%s \\% of mislabels by the SVM is shared by all models.\" % len(intersection)/len(mislabels_svm)*100)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8230433a90deedfd204f402afa77435b9f1612df21a8fe919bc9fb2fb8e7ebcb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('dat405')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
